{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from definitions import path_join, make_directory, EXPERIMENTS_DIR, VENSIM_MODELS_DIR, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_MODEL = 'known model'\n",
    "UNKNOWN_MODEL = 'unknown model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'teacup'\n",
    "experiment_name = '{}_recovery'.format(model_name)\n",
    "\n",
    "mode = UNKNOWN_MODEL\n",
    "need_retrain = False\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "general_params = {\n",
    "    'phi_h': tf.keras.activations.linear,\n",
    "    'phi_o': tf.keras.activations.linear,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'learning_rate': 1e-1,\n",
    "    'epochs_before_decay': 0.1,\n",
    "    'epochs_count': 50,\n",
    "    'learning_rate_decay': 1/3,\n",
    "    'iterations_count': 300,\n",
    "    'early_stopping_patience': 25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_enviroment(model_name, experiment_name, mode):\n",
    "    experiment_settings = dict()\n",
    "    \n",
    "    experiment_settings['model_name'] = model_name\n",
    "    experiment_settings['experiment_name'] = experiment_name\n",
    "    experiment_settings['mode'] = mode\n",
    "    \n",
    "    experiment_dir = path_join(EXPERIMENTS_DIR, experiment_name)\n",
    "    make_directory(experiment_dir)\n",
    "    experiment_settings['experiment_dir'] = experiment_dir\n",
    "    \n",
    "    tf_model_dir = path_join(experiment_dir, 'tf_model')\n",
    "    make_directory(tf_model_dir)\n",
    "    experiment_settings['tf_model_dir'] = tf_model_dir\n",
    "    \n",
    "    images_dir = path_join(experiment_dir, 'images')\n",
    "    make_directory(images_dir)\n",
    "    experiment_settings['images_dir'] = images_dir\n",
    "    \n",
    "    log_path = path_join(experiment_dir, 'log.log')\n",
    "    logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "    experiment_settings['log_path'] = log_path\n",
    "\n",
    "    vensim_model_file = path_join(VENSIM_MODELS_DIR, '{}.mdl'.format(model_name))\n",
    "    experiment_settings['vensim_model_file'] = vensim_model_file\n",
    "    \n",
    "    prn_model_dir = path_join(tf_model_dir, 'prn_model')\n",
    "    nn_model_dir = path_join(tf_model_dir, 'base_nn_model')\n",
    "    nn_2l_model_dir = path_join(tf_model_dir, 'nn_2l_model')\n",
    "    make_directory(prn_model_dir)\n",
    "    make_directory(nn_model_dir)\n",
    "    make_directory(nn_2l_model_dir)\n",
    "    \n",
    "    experiment_settings['prn_model_dir'] = prn_model_dir\n",
    "    experiment_settings['nn_model_dir'] = nn_model_dir\n",
    "    experiment_settings['nn_2l_model_dir'] = nn_2l_model_dir\n",
    "\n",
    "    return experiment_settings\n",
    "    \n",
    "experiment_settings = create_experiment_enviroment(model_name, experiment_name, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from definitions import path_join, DATA_DIR\n",
    "\n",
    "dataset_dir = path_join(DATA_DIR, model_name)\n",
    "dataset_file = path_join(dataset_dir, 'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(dataset_file)\n",
    "dt = 0.03125\n",
    "stopwords = ['TIME', 'sim_index']\n",
    "fields = [column for column in data.columns if column not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: 0.03125\n"
     ]
    }
   ],
   "source": [
    "from module.fd_model.vensim_fd_converter import create_unknown_model\n",
    "\n",
    "FD = create_unknown_model(fields)\n",
    "FD.dT = dt\n",
    "\n",
    "print('dt: {}'.format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characteristic Time', 'Room Temperature', 'Teacup Temperature']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [level for level in FD.names_units_map.keys()]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def np_preproc_for_rnn3d(numpy_array, fields):\n",
    "    x_groups = [group[1][fields].values[:-1] for group in numpy_array]\n",
    "    y_groups = [group[1][fields].values[1:] for group in numpy_array]\n",
    "    \n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(x_groups, y_groups, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    train_X = np.concatenate(train_X, axis=0)\n",
    "    valid_X = np.concatenate(valid_X, axis=0)\n",
    "    \n",
    "    train_y = np.concatenate(train_y, axis=0)\n",
    "    valid_y = np.concatenate(valid_y, axis=0)\n",
    "\n",
    "    return (train_X, train_y), (valid_X, valid_y)\n",
    "\n",
    "\n",
    "def generate_train_data(df, fields):\n",
    "    dataset = df[fields].values\n",
    "    grouped = df.groupby(['sim_index'])[fields]\n",
    "    \n",
    "    return dataset, np_preproc_for_rnn3d(grouped, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (prn_train, prn_valid) = generate_train_data(data, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn_train_X, prn_train_y = prn_train\n",
    "prn_valid_X, prn_valid_y = prn_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prn_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.fd_model.fd_rnn_converter import FDRNNConverter\n",
    "# from module.nn_model import NNModel as NNModelv1\n",
    "from module.nn_model_tf_v2 import NNModel as NNModelv2\n",
    "from module.nn_model_with_regularizer import NNModel as NNModelv3\n",
    "\n",
    "choosed_model = NNModelv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDRNN_converter = FDRNNConverter(general_params['phi_h'], general_params['phi_o'])\n",
    "rnn_model = FDRNN_converter.fd_to_rnn(FD, choosed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characteristic Time', 'Room Temperature', 'Teacup Temperature']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FD.levels\n",
    "# FD.constants\n",
    "# FD.rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_retrain:\n",
    "    rnn_model.train(prn_train_X, prn_train_y, (prn_valid_X, prn_valid_y), train_params, experiment_settings['prn_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.load(experiment_settings['prn_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(gate, w, fields, rate_names):\n",
    "    eps = 1e-4\n",
    "    equations = []\n",
    "    for col_idx in range(gate.shape[1]):\n",
    "        equation = ''\n",
    "        for row_idx in range(gate.shape[0]):\n",
    "            if (np.abs(w[row_idx, col_idx]) - eps < 0):\n",
    "                continue\n",
    "            if gate[row_idx, col_idx] == 0:\n",
    "                if w[row_idx, col_idx] > 0:\n",
    "                    sign = '*' # if w[row_idx, col_idx] > 0 else '/'\n",
    "                    weight = np.abs(w[row_idx, col_idx])\n",
    "                    if weight == 1:\n",
    "                        equation += '{}{}'.format(sign, fields[row_idx])\n",
    "                    else:\n",
    "                        equation += '{}{}^{:.1f}'.format(sign, fields[row_idx], weight)\n",
    "                else:\n",
    "                    continue\n",
    "        for row_idx in range(gate.shape[0]):\n",
    "            if (np.abs(w[row_idx, col_idx]) - eps < 0):\n",
    "                continue\n",
    "            if gate[row_idx, col_idx] == 0:\n",
    "                if w[row_idx, col_idx] <= 0:\n",
    "                    sign = '/' # if w[row_idx, col_idx] > 0 else '/'\n",
    "                    weight = np.abs(w[row_idx, col_idx])\n",
    "                    if weight == 1:\n",
    "                        equation += '{}{}'.format(sign, fields[row_idx])\n",
    "                    else:\n",
    "                        equation += '{}{}^{:.1f}'.format(sign, fields[row_idx], weight)\n",
    "                else:\n",
    "                    continue\n",
    "        for row_idx in range(gate.shape[0]):\n",
    "            if (w[row_idx, col_idx] == 0):\n",
    "                continue\n",
    "            if gate[row_idx, col_idx] == 1:\n",
    "                sign = '+' if w[row_idx, col_idx] > 0 else '-'\n",
    "                weight = np.abs(w[row_idx, col_idx])\n",
    "                if weight == 1:\n",
    "                    equation += '{}{}'.format(sign, fields[row_idx])\n",
    "                else:\n",
    "                    equation += '{}{:.1f}{}'.format(sign, weight, fields[row_idx])\n",
    "\n",
    "        equations.append((rate_names[col_idx], equation[1:]))\n",
    "    return equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_names = [rate.name for rate in FD.rates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in_Characteristic Time',\n",
       " 'out_Characteristic Time',\n",
       " 'in_Room Temperature',\n",
       " 'out_Room Temperature',\n",
       " 'in_Teacup Temperature',\n",
       " 'out_Teacup Temperature']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characteristic Time', 'Room Temperature', 'Teacup Temperature']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tf_round(x, decimals = 0):\n",
    "    multiplier = tf.constant(10**decimals, dtype=x.dtype)\n",
    "    return tf.math.round(x * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in_Characteristic Time', ''),\n",
       " ('out_Characteristic Time', ''),\n",
       " ('in_Room Temperature', ''),\n",
       " ('out_Room Temperature', ''),\n",
       " ('in_Teacup Temperature',\n",
       "  'Characteristic Time^0.5/Teacup Temperature^0.4+0.5Room Temperature'),\n",
       " ('out_Teacup Temperature',\n",
       "  'Characteristic Time^0.8/Room Temperature^0.3+0.4Teacup Temperature')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gate = np.array(my_tf_round(rnn_model.model.gate, 0), dtype=np.float32)\n",
    "_w = np.array(my_tf_round(rnn_model.model.W_ah, 1), dtype=np.float32)\n",
    "# print(fields)\n",
    "parse_weights(_gate, _w, fields, rate_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 70., 40.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_value = np.reshape(prn_valid_X[0], [1, prn_valid_X.shape[1]])\n",
    "initial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations_count = train_params['iterations_count']\n",
    "if iterations_count == 0:\n",
    "    iterations_count = X.shape[0] - 1\n",
    "iterations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn_iterative = rnn_model.get_simulation(initial_value, iterations_count, experiment_settings['prn_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prn_iterative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.base_nn import BaseNN\n",
    "from arch.base_nn_2layers import BaseNN2Layers\n",
    "\n",
    "predictor = BaseNN2Layers(prn_train_X.shape[1], prn_train_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 1s 108us/sample - loss: 2422.3589 - mse: 2422.3584 - val_loss: 730.2563 - val_mse: 730.2565\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 415.6646 - mse: 415.6647 - val_loss: 286.1596 - val_mse: 286.1596\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 300.6649 - mse: 300.6650 - val_loss: 256.4600 - val_mse: 256.4599\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 276.8381 - mse: 276.8381 - val_loss: 237.6209 - val_mse: 237.6209\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 253.6700 - mse: 253.6699 - val_loss: 216.7729 - val_mse: 216.7730\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 71us/sample - loss: 224.2754 - mse: 224.2753 - val_loss: 185.7588 - val_mse: 185.7588\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 183.5215 - mse: 183.5215 - val_loss: 142.8503 - val_mse: 142.8503\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 78us/sample - loss: 132.6789 - mse: 132.6789 - val_loss: 95.2070 - val_mse: 95.2069\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 82.6493 - mse: 82.6493 - val_loss: 55.6807 - val_mse: 55.6807\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 68us/sample - loss: 45.0870 - mse: 45.0870 - val_loss: 28.2623 - val_mse: 28.2623\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 22.6113 - mse: 22.6113 - val_loss: 13.7131 - val_mse: 13.7131\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 11.1629 - mse: 11.1629 - val_loss: 7.0940 - val_mse: 7.0940\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 67us/sample - loss: 6.0299 - mse: 6.0299 - val_loss: 4.4009 - val_mse: 4.4009\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 71us/sample - loss: 4.0207 - mse: 4.0207 - val_loss: 3.4752 - val_mse: 3.4752\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 3.3311 - mse: 3.3311 - val_loss: 3.1827 - val_mse: 3.1827\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 3.0883 - mse: 3.0883 - val_loss: 3.0610 - val_mse: 3.0610\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 2.9525 - mse: 2.9525 - val_loss: 2.9402 - val_mse: 2.9402\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 1s 75us/sample - loss: 2.8305 - mse: 2.8305 - val_loss: 2.8154 - val_mse: 2.8154\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 2.6964 - mse: 2.6964 - val_loss: 2.6754 - val_mse: 2.6754\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 77us/sample - loss: 2.5558 - mse: 2.5558 - val_loss: 2.5209 - val_mse: 2.5209\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 2.4060 - mse: 2.4060 - val_loss: 2.3689 - val_mse: 2.3689\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 2.2482 - mse: 2.2482 - val_loss: 2.2264 - val_mse: 2.2264\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 2.0956 - mse: 2.0956 - val_loss: 2.0465 - val_mse: 2.0465\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 1.9381 - mse: 1.9381 - val_loss: 1.8993 - val_mse: 1.8993\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 1.7803 - mse: 1.7803 - val_loss: 1.7588 - val_mse: 1.7588\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 1.6300 - mse: 1.6300 - val_loss: 1.5812 - val_mse: 1.5812\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 1.4850 - mse: 1.4850 - val_loss: 1.4561 - val_mse: 1.4561\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 1.3421 - mse: 1.3421 - val_loss: 1.2950 - val_mse: 1.2950\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 1.2060 - mse: 1.2060 - val_loss: 1.1624 - val_mse: 1.1624\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 1.0801 - mse: 1.0801 - val_loss: 1.0438 - val_mse: 1.0438\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 0.9615 - mse: 0.9615 - val_loss: 0.9263 - val_mse: 0.9263\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 77us/sample - loss: 0.8492 - mse: 0.8492 - val_loss: 0.8111 - val_mse: 0.8111\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 0.7453 - mse: 0.7453 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s 75us/sample - loss: 0.6477 - mse: 0.6477 - val_loss: 0.6109 - val_mse: 0.6109\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 0.5599 - mse: 0.5599 - val_loss: 0.5233 - val_mse: 0.5233\n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 1s 71us/sample - loss: 0.4781 - mse: 0.4781 - val_loss: 0.4456 - val_mse: 0.4456\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s 70us/sample - loss: 0.4044 - mse: 0.4044 - val_loss: 0.3758 - val_mse: 0.3758\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 0.3391 - mse: 0.3391 - val_loss: 0.3112 - val_mse: 0.3112\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 72us/sample - loss: 0.2797 - mse: 0.2797 - val_loss: 0.2551 - val_mse: 0.2551\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 76us/sample - loss: 0.2273 - mse: 0.2273 - val_loss: 0.2073 - val_mse: 0.2073\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s 77us/sample - loss: 0.1826 - mse: 0.1826 - val_loss: 0.1816 - val_mse: 0.1816\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 79us/sample - loss: 0.1090 - mse: 0.1090 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s 78us/sample - loss: 0.0814 - mse: 0.0814 - val_loss: 0.0707 - val_mse: 0.0707\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 76us/sample - loss: 0.0589 - mse: 0.0589 - val_loss: 0.0610 - val_mse: 0.0610\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 75us/sample - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s 77us/sample - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 74us/sample - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 73us/sample - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0037 - val_mse: 0.0037\n"
     ]
    }
   ],
   "source": [
    "if need_train:\n",
    "    predictor.train(prn_train_X, prn_train_y, (prn_valid_X, prn_valid_y), train_params, experiment_settings['nn_2l_model_dir'])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_output = predictor.test(prn_train_X, experiment_settings['nn_2l_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.06158 ,  55.009342, 138.93025 ],\n",
       "       [ 10.059795,  55.00929 , 137.88356 ],\n",
       "       [ 10.058028,  55.009235, 136.84673 ],\n",
       "       [ 10.056278,  55.00919 , 135.8198  ],\n",
       "       [ 10.054562,  55.009144, 134.81259 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.calculate_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(required_columns_data, output):\n",
    "    output = np.array(output)\n",
    "    error = sum(abs((output-required_columns_data)/required_columns_data))/required_columns_data.shape[0]\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train set error</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.87314815, 1.64409017])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_value = np.reshape(prn_train_X[0], [1, prn_train_X.shape[1]])\n",
    "train_prn_output = rnn_model.get_simulation(initial_value, iterations_count, experiment_settings['prn_model_dir'])\n",
    "train_prn_error = calculate_error(prn_train_y[:iterations_count], train_prn_output[1:])\n",
    "train_prn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = np.reshape(prn_train_X[0], [1, prn_train_X.shape[1]])\n",
    "train_nn_output = predictor.get_simulation(initial_value, iterations_count, experiment_settings['prn_model_dir'])\n",
    "train_nn_error = calculate_error(prn_train_y[:iterations_count], train_nn_output[1:])\n",
    "train_nn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 5.85885579e-05])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prn_output = rnn_model.test(prn_train_X, experiment_settings['prn_model_dir'])\n",
    "train_prn_error = calculate_error(prn_train_y, train_prn_output)\n",
    "train_prn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characteristic Time', 'Room Temperature', 'Teacup Temperature']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.66896703, 1.67173387, 0.59876746])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn_output = predictor.test(prn_train_X, experiment_settings['nn_model_dir'])\n",
    "train_nn_error = calculate_error(prn_train_y, train_nn_output)\n",
    "train_nn_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test set error</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.73180354, 1.45280095, 0.60584841])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prn_output = predictor.test(prn_valid_X, experiment_settings['nn_model_dir'])\n",
    "prn_error = calculate_error(prn_valid_y, prn_output)\n",
    "prn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 5.73139759e-05])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_nn_output = rnn_model.test(prn_valid_X, experiment_settings['nn_model_dir'])\n",
    "valid_nn_error = calculate_error(prn_valid_y, valid_nn_output)\n",
    "valid_nn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.lstm import LSTMModel\n",
    "\n",
    "sequence_size = 5\n",
    "lstm_model_dir = experiment_settings['nn_model_dir'] + '_lstm_{}seq'.format(sequence_size)\n",
    "\n",
    "predictor = LSTMModel((sequence_size, prn_train_X.shape[1]), 32, prn_train_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "c:\\users\\hwer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:5088: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  starts, ends)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def window_stack(a, stepsize=1, width=2):\n",
    "    n = a.shape[1]\n",
    "    omg = lambda i: 1+n+i-width\n",
    "    _X = [a[:, i:i+width].astype(np.float16) for i in range(0, n - width)]\n",
    "    _y = [a[:, i].astype(np.float16) for i in range(width, n)]\n",
    "    del a\n",
    "    _X = np.stack(_X, axis=1).astype(np.float16)\n",
    "    _y = np.stack(_y, axis=1).astype(np.float16)\n",
    "\n",
    "    return _X.reshape(-1, _X.shape[2], _X.shape[3]), _y.reshape(-1, _y.shape[2])\n",
    "\n",
    "def get_X_y(data, width=7):\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    X = np.array(list(data.groupby('sim_index').apply(pd.DataFrame.as_matrix)))\n",
    "    X, y = window_stack(X, width=width)\n",
    "    X = X[:, :, 1:]\n",
    "    y = y[:, 1:]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "train_ids, valid_ids = train_test_split(data['sim_index'].unique(), test_size=0.2, random_state=123)\n",
    "rnn_train = data[data['sim_index'].isin(train_ids)]\n",
    "rnn_train_X, rnn_train_y = get_X_y(rnn_train, sequence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.000e+01, 3.000e+01, 1.500e-02, 1.010e+00, 1.250e+00, 1.000e-02],\n",
       "       [2.947e+01, 3.089e+01, 1.500e-02, 1.010e+00, 1.250e+00, 1.000e-02],\n",
       "       [2.897e+01, 3.181e+01, 1.500e-02, 1.010e+00, 1.250e+00, 1.000e-02],\n",
       "       [2.848e+01, 3.278e+01, 1.500e-02, 1.010e+00, 1.250e+00, 1.000e-02],\n",
       "       [2.803e+01, 3.375e+01, 1.500e-02, 1.010e+00, 1.250e+00, 1.000e-02]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_valid = data[data['sim_index'].isin(valid_ids)]\n",
    "rnn_valid_X, rnn_valid_y = get_X_y(rnn_valid, sequence_size)\n",
    "rnn_valid_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473600 samples, validate on 118400 samples\n",
      "Epoch 1/50\n",
      "473600/473600 [==============================] - 54s 114us/sample - loss: 17171.9019 - mse: 17171.9570 - val_loss: 10123.0150 - val_mse: 10122.9727\n",
      "Epoch 2/50\n",
      "473600/473600 [==============================] - 54s 115us/sample - loss: 7680.9649 - mse: 7680.9717 - val_loss: 4630.6640 - val_mse: 4630.6572\n",
      "Epoch 3/50\n",
      "473600/473600 [==============================] - 52s 109us/sample - loss: 3643.9718 - mse: 3643.9575 - val_loss: 2127.2930 - val_mse: 2127.2935\n",
      "Epoch 4/50\n",
      "473600/473600 [==============================] - 47s 98us/sample - loss: 1722.7693 - mse: 1722.7734 - val_loss: 980.3012 - val_mse: 980.3013\n",
      "Epoch 5/50\n",
      "473600/473600 [==============================] - 50s 106us/sample - loss: 971.3384 - mse: 971.3382 - val_loss: 692.0197 - val_mse: 692.0197\n",
      "Epoch 6/50\n",
      "473600/473600 [==============================] - 46s 96us/sample - loss: 619.2739 - mse: 619.2742 - val_loss: 378.2780 - val_mse: 378.2783\n",
      "Epoch 7/50\n",
      "473600/473600 [==============================] - 51s 108us/sample - loss: 591.5671 - mse: 591.5682 - val_loss: 473.9982 - val_mse: 473.9980\n",
      "Epoch 8/50\n",
      "473600/473600 [==============================] - 46s 96us/sample - loss: 429.2422 - mse: 429.2429 - val_loss: 316.4777 - val_mse: 316.4779\n",
      "Epoch 9/50\n",
      "473600/473600 [==============================] - 45s 96us/sample - loss: 358.3189 - mse: 358.3189 - val_loss: 260.3696 - val_mse: 260.3691\n",
      "Epoch 10/50\n",
      "473600/473600 [==============================] - 47s 99us/sample - loss: 312.9213 - mse: 312.9220 - val_loss: 235.9665 - val_mse: 235.9665\n",
      "Epoch 11/50\n",
      "473600/473600 [==============================] - 50s 106us/sample - loss: 302.8178 - mse: 302.8182 - val_loss: 242.1567 - val_mse: 242.1569\n",
      "Epoch 12/50\n",
      "473600/473600 [==============================] - 52s 109us/sample - loss: 367.3912 - mse: 367.3911 - val_loss: 281.3949 - val_mse: 281.3951\n",
      "Epoch 13/50\n",
      "473600/473600 [==============================] - 54s 114us/sample - loss: 249.4710 - mse: 249.4706 - val_loss: 193.2451 - val_mse: 193.2449\n",
      "Epoch 14/50\n",
      "473600/473600 [==============================] - 48s 101us/sample - loss: 234.4633 - mse: 234.4637 - val_loss: 153.0140 - val_mse: 153.0140\n",
      "Epoch 15/50\n",
      "473600/473600 [==============================] - 47s 100us/sample - loss: 244.9070 - mse: 244.9076 - val_loss: 257.6792 - val_mse: 257.6794\n",
      "Epoch 16/50\n",
      "473600/473600 [==============================] - 48s 102us/sample - loss: 275.4149 - mse: 275.4149 - val_loss: 208.8930 - val_mse: 208.8928\n",
      "Epoch 17/50\n",
      "473600/473600 [==============================] - 48s 102us/sample - loss: 281.0663 - mse: 281.0657 - val_loss: 224.7088 - val_mse: 224.7092\n",
      "Epoch 18/50\n",
      "473600/473600 [==============================] - 47s 100us/sample - loss: 240.3435 - mse: 240.3434 - val_loss: 125.0667 - val_mse: 125.0668\n",
      "Epoch 19/50\n",
      "473600/473600 [==============================] - 47s 99us/sample - loss: 194.8536 - mse: 194.8541 - val_loss: 171.8097 - val_mse: 171.8098\n",
      "Epoch 20/50\n",
      "473600/473600 [==============================] - 49s 103us/sample - loss: 218.5103 - mse: 218.5103 - val_loss: 263.7385 - val_mse: 263.7384\n",
      "Epoch 21/50\n",
      "473600/473600 [==============================] - 51s 107us/sample - loss: 224.5850 - mse: 224.5846 - val_loss: 159.8263 - val_mse: 159.8262\n",
      "Epoch 22/50\n",
      "473600/473600 [==============================] - 50s 105us/sample - loss: 199.9581 - mse: 199.9583 - val_loss: 173.5969 - val_mse: 173.5970\n",
      "Epoch 23/50\n",
      "473600/473600 [==============================] - 50s 106us/sample - loss: 220.7578 - mse: 220.7572 - val_loss: 166.0843 - val_mse: 166.0842\n",
      "Epoch 24/50\n",
      "473600/473600 [==============================] - 49s 104us/sample - loss: 192.3740 - mse: 192.3739 - val_loss: 215.7295 - val_mse: 215.7297\n",
      "Epoch 25/50\n",
      "473600/473600 [==============================] - 49s 104us/sample - loss: 257.4282 - mse: 257.4291 - val_loss: 295.8757 - val_mse: 295.8756\n",
      "Epoch 26/50\n",
      "473600/473600 [==============================] - 48s 101us/sample - loss: 281.3173 - mse: 281.3172 - val_loss: 140.8193 - val_mse: 140.8192\n",
      "Epoch 27/50\n",
      "473600/473600 [==============================] - 52s 109us/sample - loss: 294.9979 - mse: 294.9982 - val_loss: 229.7421 - val_mse: 229.7423\n",
      "Epoch 28/50\n",
      "473600/473600 [==============================] - 51s 108us/sample - loss: 181.6464 - mse: 181.6463 - val_loss: 234.7698 - val_mse: 234.7698\n",
      "Epoch 29/50\n",
      "473600/473600 [==============================] - 50s 106us/sample - loss: 223.2531 - mse: 223.2532 - val_loss: 160.1017 - val_mse: 160.1015\n",
      "Epoch 30/50\n",
      "473600/473600 [==============================] - 52s 110us/sample - loss: 247.5915 - mse: 247.5916 - val_loss: 278.8561 - val_mse: 278.8563\n",
      "Epoch 31/50\n",
      "473600/473600 [==============================] - 52s 109us/sample - loss: 287.3478 - mse: 287.3481 - val_loss: 234.9222 - val_mse: 234.9222\n",
      "Epoch 32/50\n",
      "473600/473600 [==============================] - 51s 107us/sample - loss: 301.0131 - mse: 301.0126 - val_loss: 219.7320 - val_mse: 219.7320\n",
      "Epoch 33/50\n",
      "473600/473600 [==============================] - 51s 108us/sample - loss: 217.3187 - mse: 217.3188 - val_loss: 178.0168 - val_mse: 178.0165\n",
      "Epoch 34/50\n",
      "473600/473600 [==============================] - 47s 100us/sample - loss: 235.6591 - mse: 235.6590 - val_loss: 200.6316 - val_mse: 200.6316\n",
      "Epoch 35/50\n",
      "473600/473600 [==============================] - 48s 101us/sample - loss: 247.2041 - mse: 247.2043 - val_loss: 196.1869 - val_mse: 196.1869\n",
      "Epoch 36/50\n",
      "473600/473600 [==============================] - 49s 104us/sample - loss: 217.9728 - mse: 217.9729 - val_loss: 183.4764 - val_mse: 183.4762\n",
      "Epoch 37/50\n",
      "473600/473600 [==============================] - 50s 106us/sample - loss: 235.7970 - mse: 235.7967 - val_loss: 196.5772 - val_mse: 196.5771\n",
      "Epoch 38/50\n",
      "473600/473600 [==============================] - 52s 110us/sample - loss: 189.5811 - mse: 189.5816 - val_loss: 146.9147 - val_mse: 146.9145\n",
      "Epoch 39/50\n",
      "473600/473600 [==============================] - 51s 108us/sample - loss: 199.8348 - mse: 199.8349 - val_loss: 182.5326 - val_mse: 182.5326\n",
      "Epoch 40/50\n",
      "473600/473600 [==============================] - 49s 103us/sample - loss: 186.9201 - mse: 186.9198 - val_loss: 261.7065 - val_mse: 261.7065\n",
      "Epoch 41/50\n",
      "473600/473600 [==============================] - 50s 105us/sample - loss: 261.2613 - mse: 261.2612 - val_loss: 284.0043 - val_mse: 284.0044\n",
      "Epoch 42/50\n",
      "473600/473600 [==============================] - 49s 104us/sample - loss: 260.7995 - mse: 260.8003 - val_loss: 256.0192 - val_mse: 256.0190\n",
      "Epoch 43/50\n",
      "473600/473600 [==============================] - 50s 105us/sample - loss: 286.6135 - mse: 286.6133 - val_loss: 231.5179 - val_mse: 231.5180\n"
     ]
    }
   ],
   "source": [
    "if need_train:\n",
    "    predictor.train(rnn_train_X, rnn_train_y, (rnn_valid_X, rnn_valid_y), train_params, lstm_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
